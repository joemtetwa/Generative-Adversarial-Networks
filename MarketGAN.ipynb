{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MarketGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Zum99rg91H"
      },
      "source": [
        "# Why GAN for stock market prediction?\n",
        "\n",
        "Generative Adversarial Networks (GAN) have been recently used mainly in creating realistic images, paintings, and video clips. There aren‚Äôt many applications of GANs being used for predicting time-series data as in our case. The main idea, however, should be same ‚Äî we want to predict future stock movements. In the future, the pattern and behavior of Amazon‚Äôs stock should be more or less the same (unless it starts operating in a totally different way, or the economy drastically changes). Hence, we want to ‚Äògenerate‚Äô data for the future that will have similar (not absolutely the same, of course) distribution as the one we already have ‚Äî the historical trading data. So, in theory, it should work.\n",
        "\n",
        "**HOW DOES GAN WORK?**\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1094/1*hN0QKvuY4n07jxQCwRSmpg.jpeg)\n",
        "\n",
        "THIS BOOK IS A RESEARCH IDEA..CODE IS NOT COMPLETE...A GAN network consists of two models ‚Äî a Generator (G) and Discriminator (D). The steps in training a GAN are:\n",
        "The Generator is, using random data (noise denoted z), trying to ‚Äògenerate‚Äô data indistinguishable of, or extremely close to, the real data. Its purpose is to learn the distribution of the real data.\n",
        "\n",
        "Randomly, real or generated data is fitted into the Discriminator, which acts as a classifier and tries to understand whether the data is coming from the Generator or is the real data. D estimates the (distributions) probabilities of the incoming sample to the real dataset. (more info on comparing two distributions in section 3.2. below).\n",
        "\n",
        "\n",
        "Then, the losses from G and D are combined and propagated back through the generator. Ergo, the generator‚Äôs loss depends on both the generator and the discriminator. This is the step that helps the Generator learn about the real data distribution. If the generator doesn‚Äôt do a good job at generating a realistic data (having the same distribution), the Discriminator‚Äôs work will be very easy to distinguish generated from real data sets. Hence, the Discriminator‚Äôs loss will be very small. Small discriminator loss will result in bigger generator loss (see the equation below for L(D,G)). This makes creating the discriminator a bit tricky, because too good of a discriminator will always result in a huge generator loss, making the generator unable to learn.\n",
        "The process goes on until the Discriminator can no longer distinguish generated from real data.\n",
        "\n",
        "\n",
        "When combined together, D and G as sort of playing a minmax game (the Generator is trying to fool the Discriminator making it increase the probability for on fake examples, i.e. minimize ùîºz‚àºpz(z)[log(1‚àíD(G(z)))]. The Discriminator wants to separate the data coming from the Generator, D(G(z)), by maximizing ùîºx‚àºpr(x)[logD(x)]. Having separated loss functions, however, it is not clear how both can converge together (that is why we use some advancements over the plain GANs, such as Wasserstein GAN). Overall, the combined loss function looks like:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1094/1*TqzF5d6xFvo6IJMOdBBtnA.png)\n",
        "\n",
        "**IN OUR CASE WE WILL BE USING THE TIME SERIES GENERATED DATA IN  NOTEBOOK2 AS THE DATA AND A CNN AS THE DISCRIMINATOR**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbiDPmmc1xZz"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VNJvgTj1NoB",
        "outputId": "098f2169-d3dd-4aa9-b1bd-d0fe8286a6b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        }
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.3.0:\n",
            "  Successfully uninstalled tensorflow-2.3.0\n",
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345.2MB 54kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.2MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.35.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.18.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 368kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.10.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (50.3.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.2.0)\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, keras-applications, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGrrt16a7kR2",
        "outputId": "44fbd8e7-baca-4e07-de32-c51165f3113d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)\n",
        "#set up the google colab"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEOE8sgxjrV2",
        "outputId": "fa3df96d-6350-4bb2-d1d6-66cbadd6b725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "!wget -O xgb https://github.com/MiloMallo/StockMarketGAN/blob/master/deployed_models/xgb?raw=true"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 13:14:27--  https://github.com/MiloMallo/StockMarketGAN/blob/master/deployed_models/xgb?raw=true\n",
            "Resolving github.com (github.com)... 52.192.72.89\n",
            "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/MiloMallo/StockMarketGAN/raw/master/deployed_models/xgb [following]\n",
            "--2020-10-19 13:14:28--  https://github.com/MiloMallo/StockMarketGAN/raw/master/deployed_models/xgb\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MiloMallo/StockMarketGAN/master/deployed_models/xgb [following]\n",
            "--2020-10-19 13:14:28--  https://raw.githubusercontent.com/MiloMallo/StockMarketGAN/master/deployed_models/xgb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6205369 (5.9M) [application/octet-stream]\n",
            "Saving to: ‚Äòxgb‚Äô\n",
            "\n",
            "xgb                 100%[===================>]   5.92M  15.3MB/s    in 0.4s    \n",
            "\n",
            "2020-10-19 13:14:28 (15.3 MB/s) - ‚Äòxgb‚Äô saved [6205369/6205369]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XjSUtzcd5Bo",
        "outputId": "a40d521a-7b9f-4921-a3e4-0a89fae11387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/MiloMallo/StockMarketGAN/master/companylist.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 12:44:03--  https://raw.githubusercontent.com/MiloMallo/StockMarketGAN/master/companylist.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6417 (6.3K) [text/plain]\n",
            "Saving to: ‚Äòcompanylist.csv‚Äô\n",
            "\n",
            "companylist.csv     100%[===================>]   6.27K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-19 12:44:03 (76.7 MB/s) - ‚Äòcompanylist.csv‚Äô saved [6417/6417]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SACvOFk574rz",
        "outputId": "0c43dbc5-b110-4007-dd36-4b4b3910a9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "# drive.mount('/content/drive')\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzSA6-II8nEg"
      },
      "source": [
        "googlepath = \"/content/drive/My Drive/Colab Notebooks/MarketGAN/\"\n",
        "\n",
        "# Setting the Training Amount\n",
        "TRAINING_AMOUNT = 50000 # low to test for now\n",
        "SAVE_STEPS_AMOUNT = 10000 # testing for now\n",
        "PCT_CHANGE_AMOUNT = 5 # just want to see up down trends\n",
        "HISTORICAL_DAYS_AMOUNT = 20\n",
        "DAYS_AHEAD = 5"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1lkBfhEwc2c"
      },
      "source": [
        "## Get Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tCCJqT_9NVW",
        "outputId": "eaf9b608-d1a3-4d64-f452-a9ed844c2389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "'''\n",
        "Downloads stock data from alphavantage\n",
        "'''\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import urllib\n",
        "import json\n",
        "import csv\n",
        "import requests\n",
        "import io\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "#Key obtained From https://www.alphavantage.co/support/#api-key\n",
        "ALPHA_VANTAGE_KEY = \"QR8ARUX42GYJGT0T\" #//get your own key\n",
        "\n",
        "'''\n",
        "Note should have companylist.csv in the directory with this file.\n",
        "'''\n",
        "\n",
        "'''\n",
        "Saves data to a file\n",
        "'''\n",
        "def save(stock_csv, output_dir, filename):\n",
        "    try:\n",
        "        #the output dir may not exist\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "    except Exception as ex:\n",
        "        print('Could not create output dir')\n",
        "        print(ex)\n",
        "        return\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    try:\n",
        "#         print(stock_csv)\n",
        "        df = stock_csv\n",
        "        df = df.sort_values(by='timestamp')  \n",
        "#         print(df)\n",
        "        df.to_csv(filepath, index=False)\n",
        "    except Exception as ex:\n",
        "        print('Could not open file {} to write data'.format(filepath))\n",
        "        print(ex)\n",
        "\n",
        "\n",
        "def try_download(symbol):\n",
        "    try:\n",
        "        # Keep call frequency below threshold \n",
        "        time.sleep(12)    \n",
        "        url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey={}&datatype=csv&outputsize=full'.format(symbol, ALPHA_VANTAGE_KEY)\n",
        "        c = pd.read_csv(url)\n",
        "        # getting rid of some columns won't look at for now\n",
        "        c = c.drop(['split_coefficient', 'dividend_amount', 'adjusted_close'], axis=1)\n",
        "        return c, True\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "\n",
        "#Given a stock symbol (aka 'tsla') will download and save the data to the\n",
        "#output dir as a csv \n",
        "\n",
        "def download_symbol(symbol, output_dir, retry_count=4):\n",
        "\n",
        "    stock_csv, didPass = try_download(symbol)\n",
        "    if didPass:\n",
        "        save(stock_csv, output_dir, '{}.csv'.format(symbol))\n",
        "    else:\n",
        "        print('Failed to download {}'.format(symbol))\n",
        "\n",
        "df = pd.read_csv(f\"companylist.csv\") #f\"{googlepath}companylist.csv\")\n",
        "# df = df.sort_values(by=['MarketCap'], ascending=False)\n",
        "# Top 30 Companies\n",
        "# df = df[:30]\n",
        "df = df[:3]\n",
        "\n",
        "if not os.path.exists(f\"{googlepath}stock_data\"):\n",
        "\t    os.makedirs(f\"{googlepath}stock_data\")\n",
        "     \n",
        "for symbol in df.Symbol:\n",
        "    my_file = Path(f\"stock_data/{symbol}.csv\")  # check if already downloaded\n",
        "#     print(my_file.exists())\n",
        "    if not my_file.exists():\n",
        "        print('Downloading {}'.format(symbol))\n",
        "        download_symbol(symbol, f\"{googlepath}stock_data\")\n",
        "    else:\n",
        "        print(f\"Already downloaded {symbol}\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading AAPL\n",
            "Downloading ATVI\n",
            "Downloading ADBE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT047Jy4wmR0"
      },
      "source": [
        "## Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIy6WWcg-M5q"
      },
      "source": [
        "#plot confusion matrices\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k070Rvz8Qirt"
      },
      "source": [
        "import tensorflow as tf #Define the GAN and data generator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "SEED = 42\n",
        "tf.set_random_seed(SEED)\n",
        "\n",
        "class GAN():\n",
        "\n",
        "    def sample_Z(self, batch_size, n):\n",
        "        return np.random.uniform(-1., 1., size=(batch_size, n))\n",
        "\n",
        "    def __init__(self, num_features, num_historical_days, generator_input_size=200, is_train=True):\n",
        "        def get_batch_norm_with_global_normalization_vars(size):\n",
        "            v = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            m = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            beta = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            gamma = tf.Variable(tf.ones([size]), dtype=tf.float32)\n",
        "            return v, m, beta, gamma\n",
        "\n",
        "        self.X = tf.placeholder(tf.float32, shape=[None, num_historical_days, num_features])\n",
        "        X = tf.reshape(self.X, [-1, num_historical_days, 1, num_features])\n",
        "        self.Z = tf.placeholder(tf.float32, shape=[None, generator_input_size])\n",
        "\n",
        "        generator_output_size = num_features*num_historical_days\n",
        "        with tf.variable_scope(\"generator\"):\n",
        "            W1 = tf.Variable(tf.truncated_normal([generator_input_size, generator_output_size*10]))\n",
        "            b1 = tf.Variable(tf.truncated_normal([generator_output_size*10]))\n",
        "\n",
        "            h1 = tf.nn.sigmoid(tf.matmul(self.Z, W1) + b1)\n",
        "\n",
        "            # v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(generator_output_size*10)\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v1, m1,\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([generator_output_size*10, generator_output_size*5]))\n",
        "            b2 = tf.Variable(tf.truncated_normal([generator_output_size*5]))\n",
        "\n",
        "            h2 = tf.nn.sigmoid(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "            # v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(generator_output_size*5)\n",
        "            # h2 = tf.nn.batch_norm_with_global_normalization(h2, v2, m2,\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "\n",
        "            W3 = tf.Variable(tf.truncated_normal([generator_output_size*5, generator_output_size]))\n",
        "            b3 = tf.Variable(tf.truncated_normal([generator_output_size]))\n",
        "\n",
        "            g_log_prob = tf.matmul(h2, W3) + b3\n",
        "            g_log_prob = tf.reshape(g_log_prob, [-1, num_historical_days, 1, num_features])\n",
        "            self.gen_data = tf.reshape(g_log_prob, [-1, num_historical_days, num_features])\n",
        "            #g_log_prob = g_log_prob / tf.reshape(tf.reduce_max(g_log_prob, axis=1), [-1, 1, num_features, 1])\n",
        "            #g_prob = tf.nn.sigmoid(g_log_prob)\n",
        "\n",
        "            theta_G = [W1, b1, W2, b2, W3, b3]\n",
        "\n",
        "\n",
        "\n",
        "        with tf.variable_scope(\"discriminator\"):\n",
        "            #[filter_height, filter_width, in_channels, out_channels]\n",
        "            k1 = tf.Variable(tf.truncated_normal([3, 1, num_features, 32],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b1 = tf.Variable(tf.zeros([32], dtype=tf.float32))\n",
        "\n",
        "            v1, m1, beta1, gamma1 = get_batch_norm_with_global_normalization_vars(32)\n",
        "\n",
        "            k2 = tf.Variable(tf.truncated_normal([3, 1, 32, 64],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b2 = tf.Variable(tf.zeros([64], dtype=tf.float32))\n",
        "\n",
        "            v2, m2, beta2, gamma2 = get_batch_norm_with_global_normalization_vars(64)\n",
        "\n",
        "            k3 = tf.Variable(tf.truncated_normal([3, 1, 64, 128],\n",
        "                stddev=0.1,seed=SEED, dtype=tf.float32))\n",
        "            b3 = tf.Variable(tf.zeros([128], dtype=tf.float32))\n",
        "\n",
        "            v3, m3, beta3, gamma3 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W1 = tf.Variable(tf.truncated_normal([18*1*128, 128]))\n",
        "            b4 = tf.Variable(tf.truncated_normal([128]))\n",
        "\n",
        "            v4, m4, beta4, gamma4 = get_batch_norm_with_global_normalization_vars(128)\n",
        "\n",
        "            W2 = tf.Variable(tf.truncated_normal([128, 1]))\n",
        "\n",
        "            theta_D = [k1, b1, k2, b2, k3, b3, W1, b4, W2]\n",
        "\n",
        "        def discriminator(X):\n",
        "            conv = tf.nn.conv2d(X,k1,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b1))\n",
        "            pool = relu\n",
        "            # pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v1, m1,\n",
        "            #         beta1, gamma1, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(pool)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k2,strides=[1, 1, 1, 1],padding='SAME')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b2))\n",
        "            pool = relu\n",
        "            #pool = tf.nn.avg_pool(relu, ksize=[1, 2, 1, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "            if is_train:\n",
        "                pool = tf.nn.dropout(pool, keep_prob = 0.8)\n",
        "            # pool = tf.nn.batch_norm_with_global_normalization(pool, v2, m2,\n",
        "            #         beta2, gamma2, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(pool)\n",
        "\n",
        "            conv = tf.nn.conv2d(pool, k3, strides=[1, 1, 1, 1], padding='VALID')\n",
        "            relu = tf.nn.relu(tf.nn.bias_add(conv, b3))\n",
        "            if is_train:\n",
        "                relu = tf.nn.dropout(relu, keep_prob=0.8)\n",
        "            # relu = tf.nn.batch_norm_with_global_normalization(relu, v3, m3,\n",
        "            #         beta3, gamma3, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "            print(relu)\n",
        "\n",
        "\n",
        "            flattened_convolution_size = int(relu.shape[1]) * int(relu.shape[2]) * int(relu.shape[3])\n",
        "            print(flattened_convolution_size)\n",
        "            flattened_convolution = features = tf.reshape(relu, [-1, flattened_convolution_size])\n",
        "\n",
        "            if is_train:\n",
        "                flattened_convolution =  tf.nn.dropout(flattened_convolution, keep_prob=0.8)\n",
        "\n",
        "            h1 = tf.nn.relu(tf.matmul(flattened_convolution, W1) + b4)\n",
        "\n",
        "            # h1 = tf.nn.batch_norm_with_global_normalization(h1, v4, m4,\n",
        "            #         beta4, gamma4, variance_epsilon=0.000001, scale_after_normalization=False)\n",
        "\n",
        "            D_logit = tf.matmul(h1, W2)\n",
        "            D_prob = tf.nn.sigmoid(D_logit)\n",
        "            return D_prob, D_logit, features\n",
        "\n",
        "        D_real, D_logit_real, self.features = discriminator(X)\n",
        "        D_fake, D_logit_fake, _ = discriminator(g_log_prob)\n",
        "\n",
        "\n",
        "        D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
        "        D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
        "        self.D_l2_loss = (0.0001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_D]) / len(theta_D))\n",
        "        self.D_loss = D_loss_real + D_loss_fake + self.D_l2_loss\n",
        "        self.G_l2_loss = (0.00001 * tf.add_n([tf.nn.l2_loss(t) for t in theta_G]) / len(theta_G))\n",
        "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) + self.G_l2_loss\n",
        "\n",
        "\n",
        "        self.D_solver = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.D_loss, var_list=theta_D)\n",
        "        self.G_solver = tf.train.AdamOptimizer(learning_rate=0.000055).minimize(self.G_loss, var_list=theta_G)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag17WheKi7qV"
      },
      "source": [
        "**Defined the discriminator and generator in above code.** \n",
        "**Code ref: https://www.tensorflow.org/beta/tutorials/generative/dcgan**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4-qKPvQQ4zD",
        "outputId": "e75f9eda-0c7a-4d35-b065-f5b4536b0237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#training the GAN\n",
        "import os\n",
        "import pandas as pd\n",
        "# from gan import GAN\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(42)\n",
        "class TrainGan:\n",
        "\n",
        "    def __init__(self, num_historical_days, batch_size=128):\n",
        "        self.batch_size = batch_size\n",
        "        self.data = []\n",
        "#         files = [os.path.join('./stock_data', f) for f in os.listdir('./stock_data')]\n",
        "\n",
        "        # Google Drive Method\n",
        "        files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")]\n",
        "#         print(files)\n",
        "      \n",
        "        for file in files:\n",
        "            print(file)\n",
        "            #Read in file -- note that parse_dates will be need later\n",
        "            df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "            df = df[['open','high','low','close','volume']]\n",
        "            # #Create new index with missing days\n",
        "            # idx = pd.date_range(df.index[-1], df.index[0])\n",
        "            # #Reindex and fill the missing day with the value from the day before\n",
        "            # df = df.reindex(idx, method='bfill').sort_index(ascending=False)\n",
        "            #Normilize using a of size num_historical_days\n",
        "            df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "            #Drop the last 10 day that we don't have data for\n",
        "            df = df.dropna()\n",
        "            #Hold out the last year of trading for testing\n",
        "            #Padding to keep labels from bleeding\n",
        "            df = df[400:]\n",
        "            #This may not create good samples if num_historical_days is a\n",
        "            #mutliple of 7\n",
        "            for i in range(num_historical_days, len(df), num_historical_days):\n",
        "                self.data.append(df.values[i-num_historical_days:i])\n",
        "\n",
        "        self.gan = GAN(num_features=5, num_historical_days=num_historical_days,\n",
        "                        generator_input_size=200)\n",
        "\n",
        "    def random_batch(self, batch_size=128):\n",
        "        batch = []\n",
        "        while True:\n",
        "            batch.append(random.choice(self.data))\n",
        "            if (len(batch) == batch_size):\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "    def train(self, print_steps=100, display_data=100, save_steps=SAVE_STEPS_AMOUNT):\n",
        "        if not os.path.exists(f'{googlepath}models'):\n",
        "            os.makedirs(f'{googlepath}models')\n",
        "        sess = tf.Session()\n",
        "        \n",
        "        G_loss = 0\n",
        "        D_loss = 0\n",
        "        G_l2_loss = 0\n",
        "        D_l2_loss = 0\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        currentStep = \"0\"\n",
        "        \n",
        "        g_loss_array = []\n",
        "        d_loss_array = []\n",
        "        \n",
        "        if os.path.exists(f'{googlepath}models/checkpoint'):\n",
        "                with open(f'{googlepath}models/checkpoint', 'rb') as f:\n",
        "                    model_name = next(f).split('\"'.encode())[1]\n",
        "                filename = \"{}models/{}\".format(googlepath, model_name.decode())\n",
        "                currentStep = filename.split(\"-\")[1]\n",
        "                new_saver = tf.train.import_meta_graph('{}.meta'.format(filename))\n",
        "                new_saver.restore(sess, \"{}\".format(filename))\n",
        "\n",
        "        for i, X in enumerate(self.random_batch(self.batch_size)):\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            if i % 1 == 0:\n",
        "                _, D_loss_curr, D_l2_loss_curr = sess.run([self.gan.D_solver, self.gan.D_loss, self.gan.D_l2_loss], feed_dict=\n",
        "                        {self.gan.X:X, self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                D_loss += D_loss_curr\n",
        "                D_l2_loss += D_l2_loss_curr\n",
        "            if i % 1 == 0:\n",
        "                _, G_loss_curr, G_l2_loss_curr = sess.run([self.gan.G_solver, self.gan.G_loss, self.gan.G_l2_loss],\n",
        "                        feed_dict={self.gan.Z:self.gan.sample_Z(self.batch_size, 200)})\n",
        "                G_loss += G_loss_curr\n",
        "                G_l2_loss += G_l2_loss_curr\n",
        "                \n",
        "            g_loss_array.append(G_loss_curr - G_l2_loss)\n",
        "            d_loss_array.append(D_loss_curr - D_l2_loss)\n",
        "            \n",
        "            \n",
        "            if (i+1) % print_steps == 0:\n",
        "                print('Step={} D_loss={}, G_loss={}'.format(i + int(currentStep), D_loss/print_steps - D_l2_loss/print_steps, G_loss/print_steps - G_l2_loss/print_steps))\n",
        "                #print('D_l2_loss = {} G_l2_loss={}'.format(D_l2_loss/print_steps, G_l2_loss/print_steps))\n",
        "                G_loss = 0\n",
        "                D_loss = 0\n",
        "                G_l2_loss = 0\n",
        "                D_l2_loss = 0\n",
        "            if (i+1) % save_steps == 0:\n",
        "                saver.save(sess, f'{googlepath}/models/gan.ckpt', i + int(currentStep))\n",
        "            \n",
        "            # end training at training_amount epochs\n",
        "            if ((i + int(currentStep)) > TRAINING_AMOUNT):\n",
        "                \n",
        "                print(\"Reached {} epochs for GAN\".format(i + int(currentStep)))\n",
        "                sess.close()\n",
        "                \n",
        "                axisX = np.arange(0,len(g_loss_array),1)\n",
        "                plt.plot(axisX, g_loss_array, label='generator loss')\n",
        "                plt.plot(axisX, d_loss_array, label='discriminator loss')\n",
        "                plt.legend()\n",
        "                plt.title('generator and discriminator loss')\n",
        "                plt.show()\n",
        "                \n",
        "                break\n",
        "\n",
        "            # if (i+1) % display_data == 0:\n",
        "            #     print('Generated Data')\n",
        "            #     print(sess.run(self.gan.gen_data, feed_dict={self.gan.Z:self.gan.sample_Z(1, 200)}))\n",
        "            #     print('Real Data')\n",
        "            #     print(X[0])\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "tf.reset_default_graph()\n",
        "gan = TrainGan(HISTORICAL_DAYS_AMOUNT, 128)\n",
        "gan.train()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MarketGAN/stock_data/AAPL.csv\n",
            "/content/drive/My Drive/Colab Notebooks/MarketGAN/stock_data/ATVI.csv\n",
            "/content/drive/My Drive/Colab Notebooks/MarketGAN/stock_data/ADBE.csv\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-9-48ec8a594cea>:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Tensor(\"dropout/mul:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_1/mul:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_2/mul:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"dropout_4/mul:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"dropout_5/mul:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"dropout_6/mul:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Step=99 D_loss=38.859138298034665, G_loss=745.790326448083\n",
            "Step=199 D_loss=6.002118797302246, G_loss=813.421539722383\n",
            "Step=299 D_loss=3.5367792713642117, G_loss=752.4578636345267\n",
            "Step=399 D_loss=2.4329878556728364, G_loss=698.1714915132522\n",
            "Step=499 D_loss=1.8204917883872986, G_loss=657.9064925089478\n",
            "Step=599 D_loss=1.8493272590637206, G_loss=670.5496786364913\n",
            "Step=699 D_loss=1.579614338874817, G_loss=671.499619563818\n",
            "Step=799 D_loss=1.0253681385517122, G_loss=690.7135762178898\n",
            "Step=899 D_loss=0.6123517155647278, G_loss=709.4079006043077\n",
            "Step=999 D_loss=0.6581334030628205, G_loss=713.3234041586518\n",
            "Step=1099 D_loss=0.5879022216796876, G_loss=716.5831593000889\n",
            "Step=1199 D_loss=0.46835265040397656, G_loss=727.3383641624451\n",
            "Step=1299 D_loss=0.4422349691390992, G_loss=765.491687719524\n",
            "Step=1399 D_loss=0.30637496352195726, G_loss=753.3935097131133\n",
            "Step=1499 D_loss=0.2727324569225311, G_loss=788.1413897269964\n",
            "Step=1599 D_loss=0.29490890741348275, G_loss=820.0228111115099\n",
            "Step=1699 D_loss=0.2640676963329316, G_loss=788.9765141433477\n",
            "Step=1799 D_loss=0.20825399756431584, G_loss=750.4715663275123\n",
            "Step=1899 D_loss=0.1916174161434172, G_loss=772.9408017495274\n",
            "Step=1999 D_loss=0.15104398965835575, G_loss=777.696622838676\n",
            "Step=2099 D_loss=0.06150796175003048, G_loss=765.4305490410328\n",
            "Step=2199 D_loss=0.11347033143043506, G_loss=757.5967602059245\n",
            "Step=2299 D_loss=0.2851093995571137, G_loss=728.4562933942675\n",
            "Step=2399 D_loss=0.12423082828521737, G_loss=729.9550621715188\n",
            "Step=2499 D_loss=0.15566369056701657, G_loss=698.6135759022832\n",
            "Step=2599 D_loss=0.11944031357765206, G_loss=721.0824149858952\n",
            "Step=2699 D_loss=0.12492672204971322, G_loss=692.9917670115829\n",
            "Step=2799 D_loss=0.18698318719863893, G_loss=674.9500404158234\n",
            "Step=2899 D_loss=0.12186865210533138, G_loss=646.5314260569214\n",
            "Step=2999 D_loss=0.2735906362533571, G_loss=611.8650929784775\n",
            "Step=3099 D_loss=0.2690353250503539, G_loss=604.2651765853167\n",
            "Step=3199 D_loss=0.22195398449897752, G_loss=577.4642881706357\n",
            "Step=3299 D_loss=0.27757862925529464, G_loss=544.1442279917002\n",
            "Step=3399 D_loss=0.4159938144683839, G_loss=502.4454869166017\n",
            "Step=3499 D_loss=0.5311185562610627, G_loss=467.9851173233986\n",
            "Step=3599 D_loss=0.6688771736621857, G_loss=433.53554703146216\n",
            "Step=3699 D_loss=0.7164245092868806, G_loss=404.5663242542744\n",
            "Step=3799 D_loss=1.0732360339164735, G_loss=376.68039596378804\n",
            "Step=3899 D_loss=1.0057939517498014, G_loss=326.92779744982715\n",
            "Step=3999 D_loss=1.0338218843936922, G_loss=289.0018270269036\n",
            "Step=4099 D_loss=0.8575780963897708, G_loss=249.9385607177019\n",
            "Step=4199 D_loss=0.9781133878231048, G_loss=224.56695071130991\n",
            "Step=4299 D_loss=0.7538992607593535, G_loss=180.77863980919122\n",
            "Step=4399 D_loss=0.8741042125225065, G_loss=150.87712296426298\n",
            "Step=4499 D_loss=0.7334546864032745, G_loss=131.53322698652744\n",
            "Step=4599 D_loss=0.7204446935653688, G_loss=112.57848841547965\n",
            "Step=4699 D_loss=0.29540922999382, G_loss=108.05284517526626\n",
            "Step=4799 D_loss=0.3540208852291107, G_loss=100.39019268840552\n",
            "Step=4899 D_loss=0.20845072627067562, G_loss=89.07689777076244\n",
            "Step=4999 D_loss=0.22187226414680472, G_loss=85.2650018042326\n",
            "Step=5099 D_loss=0.1088837397098541, G_loss=81.920365819633\n",
            "Step=5199 D_loss=0.13229928374290467, G_loss=74.3090784049034\n",
            "Step=5299 D_loss=0.07566822171211252, G_loss=70.60107161194085\n",
            "Step=5399 D_loss=0.09603484630584713, G_loss=65.9205459395051\n",
            "Step=5499 D_loss=0.07580849528312683, G_loss=66.37280350506306\n",
            "Step=5599 D_loss=0.08405058145523081, G_loss=63.62517319351434\n",
            "Step=5699 D_loss=0.03936532378196711, G_loss=61.3487447565794\n",
            "Step=5799 D_loss=0.08610570907592785, G_loss=57.0919411021471\n",
            "Step=5899 D_loss=0.0650885176658631, G_loss=51.96334154367447\n",
            "Step=5999 D_loss=0.26619626283645625, G_loss=45.44369209825993\n",
            "Step=6099 D_loss=1.604703586101532, G_loss=31.798681255877018\n",
            "Step=6199 D_loss=4.07608120083809, G_loss=15.136893938779831\n",
            "Step=6299 D_loss=2.850398336648941, G_loss=11.998279981613159\n",
            "Step=6399 D_loss=2.400516226291656, G_loss=9.469077605903149\n",
            "Step=6499 D_loss=2.1622396552562715, G_loss=9.303233193457126\n",
            "Step=6599 D_loss=1.7529207062721253, G_loss=8.619912858009338\n",
            "Step=6699 D_loss=1.6731738317012788, G_loss=7.754181216657161\n",
            "Step=6799 D_loss=1.591939072608948, G_loss=6.263967683911323\n",
            "Step=6899 D_loss=1.3745778238773345, G_loss=6.747309358119964\n",
            "Step=6999 D_loss=1.871544008255005, G_loss=5.195684580206871\n",
            "Step=7099 D_loss=1.3190982234477997, G_loss=4.9667231759428985\n",
            "Step=7199 D_loss=1.220695765018463, G_loss=5.279473468065262\n",
            "Step=7299 D_loss=1.399833011627197, G_loss=5.367140143215656\n",
            "Step=7399 D_loss=1.2245440220832824, G_loss=4.814386293888092\n",
            "Step=7499 D_loss=1.7437925696372987, G_loss=4.1153093329072\n",
            "Step=7599 D_loss=1.0945520830154418, G_loss=3.5209225779771804\n",
            "Step=7699 D_loss=0.670890052318573, G_loss=3.8427464413642882\n",
            "Step=7799 D_loss=0.6712479448318482, G_loss=4.429444844424724\n",
            "Step=7899 D_loss=1.0664239001274112, G_loss=4.575496992766857\n",
            "Step=7999 D_loss=1.4173365008831023, G_loss=3.2411295801401137\n",
            "Step=8099 D_loss=1.1835627508163453, G_loss=3.1375086307525635\n",
            "Step=8199 D_loss=0.9313085806369781, G_loss=3.2333134868741036\n",
            "Step=8299 D_loss=1.7723701429367067, G_loss=3.1568504959344867\n",
            "Step=8399 D_loss=1.651048434972763, G_loss=3.6361775490641595\n",
            "Step=8499 D_loss=0.7293266236782074, G_loss=3.3572667184472085\n",
            "Step=8599 D_loss=0.8756122183799746, G_loss=3.077858365774155\n",
            "Step=8699 D_loss=0.9687812554836275, G_loss=3.0622516313195227\n",
            "Step=8799 D_loss=0.7889722311496734, G_loss=3.6697319185733797\n",
            "Step=8899 D_loss=1.0020972776412962, G_loss=3.3536164614558217\n",
            "Step=8999 D_loss=0.9080187559127808, G_loss=3.0862112405896185\n",
            "Step=9099 D_loss=1.439224534034729, G_loss=2.8537227514386174\n",
            "Step=9199 D_loss=1.4374123156070708, G_loss=3.1992829522490505\n",
            "Step=9299 D_loss=1.0365438580513002, G_loss=3.1883823496103285\n",
            "Step=9399 D_loss=1.3654239058494566, G_loss=2.4270275834202764\n",
            "Step=9499 D_loss=0.9779060673713682, G_loss=2.3616827684640884\n",
            "Step=9599 D_loss=1.0468243634700776, G_loss=2.103008958399296\n",
            "Step=9699 D_loss=0.9727884984016417, G_loss=2.9395322120189666\n",
            "Step=9799 D_loss=0.8626199436187745, G_loss=2.6252597814798353\n",
            "Step=9899 D_loss=1.8470547115802767, G_loss=1.8166789653897286\n",
            "Step=9999 D_loss=0.9080705273151397, G_loss=2.919861457645893\n",
            "Step=10099 D_loss=1.0081264436244965, G_loss=2.5293606325984\n",
            "Step=10199 D_loss=1.157031458616257, G_loss=2.0638897946476935\n",
            "Step=10299 D_loss=0.6601061892509459, G_loss=3.1366791823506355\n",
            "Step=10399 D_loss=1.2705408918857575, G_loss=1.833609945476055\n",
            "Step=10499 D_loss=1.0591824066638948, G_loss=2.361803766191006\n",
            "Step=10599 D_loss=0.7261357915401458, G_loss=2.8200636100769043\n",
            "Step=10699 D_loss=1.8226278054714202, G_loss=1.803082193136215\n",
            "Step=10799 D_loss=1.0252019500732423, G_loss=1.5914639309048653\n",
            "Step=10899 D_loss=1.0532461607456207, G_loss=1.641666118502617\n",
            "Step=10999 D_loss=0.964730534553528, G_loss=2.079793064892292\n",
            "Step=11099 D_loss=1.387149568796158, G_loss=2.628326773941517\n",
            "Step=11199 D_loss=1.7469663357734682, G_loss=2.4809358203411103\n",
            "Step=11299 D_loss=1.3756890451908113, G_loss=1.8190551686286927\n",
            "Step=11399 D_loss=0.9069095265865323, G_loss=2.131488858759403\n",
            "Step=11499 D_loss=0.8892787373065947, G_loss=2.811525454223156\n",
            "Step=11599 D_loss=1.4483200657367707, G_loss=1.7302531588077543\n",
            "Step=11699 D_loss=1.3687807321548464, G_loss=1.8525553455948827\n",
            "Step=11799 D_loss=1.331019957065582, G_loss=2.000607739090919\n",
            "Step=11899 D_loss=0.9309818959236145, G_loss=2.3117140319943426\n",
            "Step=11999 D_loss=0.8050848102569581, G_loss=2.5787952145934105\n",
            "Step=12099 D_loss=1.0071757590770722, G_loss=1.9224114197492599\n",
            "Step=12199 D_loss=1.1939958739280703, G_loss=1.7304247289896013\n",
            "Step=12299 D_loss=1.0550958335399625, G_loss=2.3487725213170054\n",
            "Step=12399 D_loss=1.790185024738312, G_loss=1.3960964521765709\n",
            "Step=12499 D_loss=1.089099509716034, G_loss=1.7951840633153917\n",
            "Step=12599 D_loss=0.9976901113986969, G_loss=1.7409283342957498\n",
            "Step=12699 D_loss=1.1299556529521944, G_loss=1.8915849262475968\n",
            "Step=12799 D_loss=1.4205008280277254, G_loss=2.182957231104374\n",
            "Step=12899 D_loss=0.8726906883716583, G_loss=2.6726525038480755\n",
            "Step=12999 D_loss=1.6699450290203095, G_loss=1.6439617526531218\n",
            "Step=13099 D_loss=0.8563248848915099, G_loss=3.2424547809362414\n",
            "Step=13199 D_loss=0.962828023433685, G_loss=2.121157185435295\n",
            "Step=13299 D_loss=1.1184508419036863, G_loss=1.9544585698843\n",
            "Step=13399 D_loss=1.1261224877834322, G_loss=1.256315416097641\n",
            "Step=13499 D_loss=1.3971222031116488, G_loss=1.702205802500248\n",
            "Step=13599 D_loss=0.963096626996994, G_loss=1.9169054502248766\n",
            "Step=13699 D_loss=0.9302234399318694, G_loss=1.6949701014161112\n",
            "Step=13799 D_loss=1.073208817243576, G_loss=1.4538105472922325\n",
            "Step=13899 D_loss=0.8272257792949678, G_loss=2.6740814065933227\n",
            "Step=13999 D_loss=1.2584878480434418, G_loss=1.8511647894978522\n",
            "Step=14099 D_loss=1.1237046694755553, G_loss=1.4121581900119782\n",
            "Step=14199 D_loss=0.8908917999267578, G_loss=1.6355940037965775\n",
            "Step=14299 D_loss=0.9398879957199098, G_loss=2.3382033696770668\n",
            "Step=14399 D_loss=0.6069022047519683, G_loss=2.1883230632543564\n",
            "Step=14499 D_loss=1.1312166368961334, G_loss=2.1713814070820807\n",
            "Step=14599 D_loss=1.2281733930110934, G_loss=2.101758618056774\n",
            "Step=14699 D_loss=0.7358510863780976, G_loss=1.7311330667138098\n",
            "Step=14799 D_loss=1.903761270046234, G_loss=1.6142892411351202\n",
            "Step=14899 D_loss=1.3123075723648072, G_loss=1.1853238293528556\n",
            "Step=14999 D_loss=1.0125771284103395, G_loss=1.5897173139452936\n",
            "Step=15099 D_loss=0.9891875410079956, G_loss=1.8139137285947802\n",
            "Step=15199 D_loss=1.3459204149246218, G_loss=2.380152489840984\n",
            "Step=15299 D_loss=1.2693919122219086, G_loss=1.1187707298994063\n",
            "Step=15399 D_loss=1.0560388803482057, G_loss=2.677969333231449\n",
            "Step=15499 D_loss=0.4541850078105927, G_loss=3.31956642895937\n",
            "Step=15599 D_loss=1.4238199388980866, G_loss=1.504490720629692\n",
            "Step=15699 D_loss=0.8248546612262724, G_loss=1.6841385886073115\n",
            "Step=15799 D_loss=1.092991713285446, G_loss=1.4650293585658074\n",
            "Step=15899 D_loss=1.4039716529846193, G_loss=2.7033264696598054\n",
            "Step=15999 D_loss=1.2736673665046692, G_loss=2.9606824663281444\n",
            "Step=16099 D_loss=0.900098363161087, G_loss=1.5004416140913963\n",
            "Step=16199 D_loss=1.255722303390503, G_loss=1.4625317615270614\n",
            "Step=16299 D_loss=1.1310584843158722, G_loss=1.6976313138008117\n",
            "Step=16399 D_loss=1.1209178411960603, G_loss=3.3612275463342667\n",
            "Step=16499 D_loss=0.8467202198505401, G_loss=2.4817688807845117\n",
            "Step=16599 D_loss=1.253460787534714, G_loss=1.2831596499681472\n",
            "Step=16699 D_loss=0.8056749439239503, G_loss=1.8926358368992804\n",
            "Step=16799 D_loss=1.052637070417404, G_loss=1.4072965469956398\n",
            "Step=16899 D_loss=1.041363937854767, G_loss=1.5447683691978455\n",
            "Step=16999 D_loss=1.143995419740677, G_loss=1.1909409487247469\n",
            "Step=17099 D_loss=1.1869193744659425, G_loss=1.4635963478684426\n",
            "Step=17199 D_loss=0.7650723767280578, G_loss=2.2810723036527634\n",
            "Step=17299 D_loss=0.8487862205505372, G_loss=1.6338071742653844\n",
            "Step=17399 D_loss=1.1139224720001222, G_loss=1.3985569125413895\n",
            "Step=17499 D_loss=1.0077756786346435, G_loss=1.9940532773733142\n",
            "Step=17599 D_loss=1.0198452937602998, G_loss=1.4701697370409965\n",
            "Step=17699 D_loss=1.3969595253467562, G_loss=2.251980922818184\n",
            "Step=17799 D_loss=0.939879122376442, G_loss=2.758216080963612\n",
            "Step=17899 D_loss=2.0259968489408493, G_loss=2.4388941144943237\n",
            "Step=17999 D_loss=1.053102466464043, G_loss=1.3288810005784035\n",
            "Step=18099 D_loss=1.0074259024858474, G_loss=1.4381344369053841\n",
            "Step=18199 D_loss=0.7947090572118759, G_loss=1.533279995620251\n",
            "Step=18299 D_loss=1.3470253556966783, G_loss=1.1518889012932778\n",
            "Step=18399 D_loss=0.9934945946931839, G_loss=2.2452683445811275\n",
            "Step=18499 D_loss=0.8729741925001145, G_loss=3.6948240333795543\n",
            "Step=18599 D_loss=1.1482099455595014, G_loss=1.0885260862112045\n",
            "Step=18699 D_loss=0.7975605404376984, G_loss=1.5887830305099486\n",
            "Step=18799 D_loss=1.100697080492973, G_loss=1.8676221668720245\n",
            "Step=18899 D_loss=0.9687578642368316, G_loss=2.2963161844015123\n",
            "Step=18999 D_loss=0.9465056931972504, G_loss=1.4757842260599137\n",
            "Step=19099 D_loss=0.6991052550077439, G_loss=1.968788875043392\n",
            "Step=19199 D_loss=1.0507711720466615, G_loss=1.5297745332121848\n",
            "Step=19299 D_loss=0.8119357550144195, G_loss=1.9419884124398232\n",
            "Step=19399 D_loss=1.174424827694893, G_loss=1.567751697897911\n",
            "Step=19499 D_loss=0.7111613702774048, G_loss=2.753319637179375\n",
            "Step=19599 D_loss=1.1470754939317704, G_loss=1.7536372554302215\n",
            "Step=19699 D_loss=1.1859082233905789, G_loss=1.4319557160139083\n",
            "Step=19799 D_loss=0.9686169081926346, G_loss=3.2313678461313247\n",
            "Step=19899 D_loss=0.7869676792621613, G_loss=2.0129042214155195\n",
            "Step=19999 D_loss=0.7853898799419403, G_loss=1.437224873304367\n",
            "Step=20099 D_loss=0.7908624535799027, G_loss=1.8745506951212885\n",
            "Step=20199 D_loss=0.6125155192613602, G_loss=1.8952361491322518\n",
            "Step=20299 D_loss=1.6036375218629835, G_loss=1.698306506574154\n",
            "Step=20399 D_loss=1.3049599593877792, G_loss=1.3490174439549447\n",
            "Step=20499 D_loss=0.8379211568832398, G_loss=2.742469331026077\n",
            "Step=20599 D_loss=1.0571347248554228, G_loss=2.2684726706147194\n",
            "Step=20699 D_loss=1.5119444900751113, G_loss=2.082076192200184\n",
            "Step=20799 D_loss=0.9434090399742127, G_loss=1.1687780237197876\n",
            "Step=20899 D_loss=0.7872514289617538, G_loss=1.9687669098377227\n",
            "Step=20999 D_loss=1.0641690069437026, G_loss=1.5226687571406368\n",
            "Step=21099 D_loss=1.106984344124794, G_loss=1.494437652826309\n",
            "Step=21199 D_loss=0.9196693980693817, G_loss=1.7373136302828789\n",
            "Step=21299 D_loss=0.9951217049360276, G_loss=4.319764541685581\n",
            "Step=21399 D_loss=0.9253187763690949, G_loss=1.9979223158955575\n",
            "Step=21499 D_loss=0.7849221444129945, G_loss=1.7704544809460638\n",
            "Step=21599 D_loss=1.4004949724674227, G_loss=2.5804087328910827\n",
            "Step=21699 D_loss=0.9779571437835693, G_loss=2.3019876211881636\n",
            "Step=21799 D_loss=0.915250317454338, G_loss=1.8480241107940674\n",
            "Step=21899 D_loss=0.624334682226181, G_loss=2.079428227543831\n",
            "Step=21999 D_loss=1.0822080367803573, G_loss=2.148518078625202\n",
            "Step=22099 D_loss=0.8264837324619292, G_loss=1.476936128139496\n",
            "Step=22199 D_loss=0.8073753589391708, G_loss=1.6132618096470834\n",
            "Step=22299 D_loss=1.0813403463363649, G_loss=1.6829473069310188\n",
            "Step=22399 D_loss=1.0209155452251433, G_loss=1.9630066823959351\n",
            "Step=22499 D_loss=0.6907266080379486, G_loss=1.4790692731738089\n",
            "Step=22599 D_loss=0.6788231265544892, G_loss=2.1967279693484305\n",
            "Step=22699 D_loss=1.4360620087385176, G_loss=2.108654876053333\n",
            "Step=22799 D_loss=1.0599021250009537, G_loss=2.067762795090675\n",
            "Step=22899 D_loss=1.0108026468753817, G_loss=1.6826495710015295\n",
            "Step=22999 D_loss=0.7159061431884766, G_loss=3.487951511144638\n",
            "Step=23099 D_loss=1.4301743376255036, G_loss=1.5864491909742355\n",
            "Step=23199 D_loss=0.6140207076072692, G_loss=1.7704363003373147\n",
            "Step=23299 D_loss=0.5750227522850035, G_loss=3.572144205570221\n",
            "Step=23399 D_loss=0.8398220831155776, G_loss=1.9267060565948486\n",
            "Step=23499 D_loss=1.0827972459793092, G_loss=1.4157123467326165\n",
            "Step=23599 D_loss=0.903135232925415, G_loss=1.3091056236624719\n",
            "Step=23699 D_loss=0.9593336427211762, G_loss=3.0940163692831995\n",
            "Step=23799 D_loss=0.7301117604970931, G_loss=2.4843642297387123\n",
            "Step=23899 D_loss=1.3511112570762633, G_loss=1.3521852692961693\n",
            "Step=23999 D_loss=0.9661649268865584, G_loss=1.8649872916936876\n",
            "Step=24099 D_loss=1.1770597130060196, G_loss=1.3371447929739952\n",
            "Step=24199 D_loss=0.8190932786464691, G_loss=1.3689470648765565\n",
            "Step=24299 D_loss=0.9417650079727173, G_loss=1.822509834766388\n",
            "Step=24399 D_loss=0.8284957379102706, G_loss=2.1759776490926743\n",
            "Step=24499 D_loss=1.506801612377167, G_loss=1.7304711934924124\n",
            "Step=24599 D_loss=1.294389234781265, G_loss=2.4904982647299767\n",
            "Step=24699 D_loss=0.6566769874095917, G_loss=2.665102737247944\n",
            "Step=24799 D_loss=1.3380850100517272, G_loss=1.4859157714247704\n",
            "Step=24899 D_loss=1.0738164395093917, G_loss=1.376907249391079\n",
            "Step=24999 D_loss=1.2786692655086518, G_loss=2.1538978430628775\n",
            "Step=25099 D_loss=0.7706988435983658, G_loss=2.958690558075905\n",
            "Step=25199 D_loss=1.0298610800504684, G_loss=1.522358908355236\n",
            "Step=25299 D_loss=0.6739105117321015, G_loss=1.7830137646198272\n",
            "Step=25399 D_loss=1.3352908623218536, G_loss=0.9393281331658364\n",
            "Step=25499 D_loss=1.1082339233160017, G_loss=1.1574781677126884\n",
            "Step=25599 D_loss=1.0166269224882127, G_loss=1.1008899560570717\n",
            "Step=25699 D_loss=0.6484968036413192, G_loss=2.5155803501605987\n",
            "Step=25799 D_loss=1.3632769817113877, G_loss=0.9407341086864471\n",
            "Step=25899 D_loss=1.0995399302244184, G_loss=1.369659025967121\n",
            "Step=25999 D_loss=1.3440364849567414, G_loss=1.4881535455584525\n",
            "Step=26099 D_loss=0.6814857971668243, G_loss=2.732939719259739\n",
            "Step=26199 D_loss=1.006672609448433, G_loss=1.5613760831952097\n",
            "Step=26299 D_loss=0.9924652254581451, G_loss=1.758649335205555\n",
            "Step=26399 D_loss=1.5581736755371094, G_loss=1.942577599585056\n",
            "Step=26499 D_loss=1.7029445004463195, G_loss=5.142383370101452\n",
            "Step=26599 D_loss=1.4058786553144453, G_loss=1.054369869530201\n",
            "Step=26699 D_loss=1.1207579207420348, G_loss=1.3088217961788178\n",
            "Step=26799 D_loss=0.8439736330509187, G_loss=1.4224106094241142\n",
            "Step=26899 D_loss=0.6246713739633561, G_loss=1.9330857819318772\n",
            "Step=26999 D_loss=1.0691289681196212, G_loss=1.6327260956168177\n",
            "Step=27099 D_loss=1.5358651757240298, G_loss=1.5179411885142327\n",
            "Step=27199 D_loss=0.6666074478626252, G_loss=1.9986680898070337\n",
            "Step=27299 D_loss=0.9521977716684341, G_loss=1.3693191349506377\n",
            "Step=27399 D_loss=0.8535716080665587, G_loss=1.4876688894629477\n",
            "Step=27499 D_loss=0.9201073741912843, G_loss=2.4737602093815805\n",
            "Step=27599 D_loss=1.1575015211105346, G_loss=2.2904484394192695\n",
            "Step=27699 D_loss=0.9501318186521531, G_loss=2.353248192667961\n",
            "Step=27799 D_loss=0.9954198127985, G_loss=1.387685741484165\n",
            "Step=27899 D_loss=0.747126346230507, G_loss=1.7977701431512831\n",
            "Step=27999 D_loss=0.9412534701824189, G_loss=1.4189258387684822\n",
            "Step=28099 D_loss=0.7261231678724288, G_loss=1.6966342166066168\n",
            "Step=28199 D_loss=1.6885351157188415, G_loss=2.4118541356921197\n",
            "Step=28299 D_loss=1.0762368708848953, G_loss=2.053237260878086\n",
            "Step=28399 D_loss=1.5899492257833479, G_loss=0.9692471057176589\n",
            "Step=28499 D_loss=0.9905866926908494, G_loss=1.892275303900242\n",
            "Step=28599 D_loss=0.8194552969932556, G_loss=2.904199242889881\n",
            "Step=28699 D_loss=1.1424056559801101, G_loss=1.8221722781658172\n",
            "Step=28799 D_loss=0.8543684899806976, G_loss=1.9941155093908312\n",
            "Step=28899 D_loss=1.0438855493068697, G_loss=1.3223291754722597\n",
            "Step=28999 D_loss=0.9295270711183548, G_loss=1.8918692755699158\n",
            "Step=29099 D_loss=0.9151973015069961, G_loss=1.4065105983614923\n",
            "Step=29199 D_loss=1.2246092498302459, G_loss=2.6817479065060614\n",
            "Step=29299 D_loss=2.030619449019432, G_loss=1.612771969139576\n",
            "Step=29399 D_loss=1.6567437702417371, G_loss=13.864118461310863\n",
            "Step=29499 D_loss=1.4044539910554885, G_loss=3.395640120506287\n",
            "Step=29599 D_loss=0.9661028957366944, G_loss=1.2463237312436104\n",
            "Step=29699 D_loss=0.9028947710990904, G_loss=1.295035325884819\n",
            "Step=29799 D_loss=0.7868956208229064, G_loss=1.3893900513648987\n",
            "Step=29899 D_loss=0.7795735836029053, G_loss=1.69313733458519\n",
            "Step=29999 D_loss=0.6900587564706803, G_loss=2.3645320665836334\n",
            "Step=30099 D_loss=1.2284148287773133, G_loss=1.5024154335260391\n",
            "Step=30199 D_loss=0.6554059225320815, G_loss=2.513839832246304\n",
            "Step=30299 D_loss=0.7123469257354735, G_loss=2.8243762683868407\n",
            "Step=30399 D_loss=0.9151821041107178, G_loss=1.319461532831192\n",
            "Step=30499 D_loss=0.9408071911334992, G_loss=1.6549229121208189\n",
            "Step=30599 D_loss=0.7943495631217957, G_loss=3.352628915905952\n",
            "Step=30699 D_loss=0.8173428863286972, G_loss=3.1844742068648335\n",
            "Step=30799 D_loss=1.3054842334985732, G_loss=1.1989269536733627\n",
            "Step=30899 D_loss=0.6288452047109605, G_loss=2.1514601305127146\n",
            "Step=30999 D_loss=0.9334371042251587, G_loss=1.9598211735486986\n",
            "Step=31099 D_loss=1.1594937229156494, G_loss=1.2227063634991646\n",
            "Step=31199 D_loss=0.7744720137119294, G_loss=1.6134126278758048\n",
            "Step=31299 D_loss=1.0994739931821824, G_loss=1.5545927950739862\n",
            "Step=31399 D_loss=1.093624920845032, G_loss=1.3892543256282806\n",
            "Step=31499 D_loss=1.3074463623762131, G_loss=1.3997844317555428\n",
            "Step=31599 D_loss=1.2616477257013319, G_loss=1.1887612414360045\n",
            "Step=31699 D_loss=0.6592492932081223, G_loss=2.9858307325839997\n",
            "Step=31799 D_loss=0.8817399042844771, G_loss=1.9755668288469317\n",
            "Step=31899 D_loss=1.2375024110078812, G_loss=1.4043294933438302\n",
            "Step=31999 D_loss=0.924766515493393, G_loss=1.3532257002592087\n",
            "Step=32099 D_loss=1.5247065085172653, G_loss=1.1833562648296356\n",
            "Step=32199 D_loss=1.0596123415231706, G_loss=3.2510085815191267\n",
            "Step=32299 D_loss=1.5740827345848083, G_loss=1.822420325875282\n",
            "Step=32399 D_loss=1.0764406257867813, G_loss=1.2758179223537445\n",
            "Step=32499 D_loss=1.0479623371362687, G_loss=1.9852926751971247\n",
            "Step=32599 D_loss=0.9390079724788665, G_loss=1.4651975533366204\n",
            "Step=32699 D_loss=0.9377217584848405, G_loss=1.8460030108690262\n",
            "Step=32799 D_loss=1.0858407217264174, G_loss=1.3879020258784296\n",
            "Step=32899 D_loss=0.8845068514347075, G_loss=1.7366121280193327\n",
            "Step=32999 D_loss=1.57954919397831, G_loss=2.2417900621891023\n",
            "Step=33099 D_loss=0.9320209300518035, G_loss=1.9260033574700355\n",
            "Step=33199 D_loss=1.076098016500473, G_loss=1.9806961959600449\n",
            "Step=33299 D_loss=0.6250608068704605, G_loss=1.8761521250009536\n",
            "Step=33399 D_loss=1.6824043589830397, G_loss=6.900108602643013\n",
            "Step=33499 D_loss=1.836605034470558, G_loss=3.2168193712830546\n",
            "Step=33599 D_loss=0.9408627218008041, G_loss=1.2271638882160185\n",
            "Step=33699 D_loss=0.9209724247455597, G_loss=1.3074383869767188\n",
            "Step=33799 D_loss=0.9247576969861984, G_loss=1.1733312493562698\n",
            "Step=33899 D_loss=0.7272594374418259, G_loss=1.3022498857975007\n",
            "Step=33999 D_loss=0.7087941938638688, G_loss=1.4795767772197723\n",
            "Step=34099 D_loss=0.8687369352579116, G_loss=3.2970001071691515\n",
            "Step=34199 D_loss=1.1699679607152937, G_loss=1.425216620862484\n",
            "Step=34299 D_loss=0.9032101207971573, G_loss=1.7481364086270332\n",
            "Step=34399 D_loss=1.1658428651094437, G_loss=2.104458501636982\n",
            "Step=34499 D_loss=0.8514532649517058, G_loss=1.8316704058647155\n",
            "Step=34599 D_loss=0.5567282122373582, G_loss=1.8589606475830078\n",
            "Step=34699 D_loss=1.116244130730629, G_loss=2.82436752140522\n",
            "Step=34799 D_loss=0.9590797835588456, G_loss=1.816181741654873\n",
            "Step=34899 D_loss=1.1670046734809876, G_loss=1.4055797910690306\n",
            "Step=34999 D_loss=1.1645700109004975, G_loss=1.9419705063104629\n",
            "Step=35099 D_loss=0.9710592001676558, G_loss=1.4216524043679237\n",
            "Step=35199 D_loss=0.9206882452964783, G_loss=1.5024919059872626\n",
            "Step=35299 D_loss=0.8872647494077682, G_loss=2.068309463560581\n",
            "Step=35399 D_loss=0.5588724005222321, G_loss=3.78039620667696\n",
            "Step=35499 D_loss=1.8582167583703995, G_loss=1.338612723648548\n",
            "Step=35599 D_loss=0.8956324821710586, G_loss=1.286158130466938\n",
            "Step=35699 D_loss=0.5339487332105636, G_loss=2.549621829986572\n",
            "Step=35799 D_loss=0.8459485638141633, G_loss=1.7182537201046946\n",
            "Step=35899 D_loss=0.7090047985315323, G_loss=2.307126867175102\n",
            "Step=35999 D_loss=1.1196698755025865, G_loss=1.3636416003108023\n",
            "Step=36099 D_loss=1.5910887658596038, G_loss=1.6521879681944847\n",
            "Step=36199 D_loss=1.05180406332016, G_loss=1.1820550245046615\n",
            "Step=36299 D_loss=0.9152167069911956, G_loss=1.2766089630126953\n",
            "Step=36399 D_loss=0.6432617408037186, G_loss=1.610572648346424\n",
            "Step=36499 D_loss=1.3115864515304567, G_loss=5.8679502117633815\n",
            "Step=36599 D_loss=1.392822634577751, G_loss=1.1086718115210532\n",
            "Step=36699 D_loss=1.0209625035524368, G_loss=1.3256691181659699\n",
            "Step=36799 D_loss=0.8332420945167541, G_loss=1.560629950463772\n",
            "Step=36899 D_loss=1.206530436873436, G_loss=1.1433930429816246\n",
            "Step=36999 D_loss=0.8640985417366028, G_loss=1.5873247143626212\n",
            "Step=37099 D_loss=1.0638285344839096, G_loss=1.4173171827197075\n",
            "Step=37199 D_loss=1.5347408878803255, G_loss=1.765815376341343\n",
            "Step=37299 D_loss=1.1515071380138395, G_loss=2.1470529717206954\n",
            "Step=37399 D_loss=1.0969619578123093, G_loss=0.9748074907064438\n",
            "Step=37499 D_loss=0.9417392426729203, G_loss=1.4410008308291435\n",
            "Step=37599 D_loss=1.0127153980731962, G_loss=1.5712447950243948\n",
            "Step=37699 D_loss=0.7388493555784226, G_loss=1.4459080857038498\n",
            "Step=37799 D_loss=1.2521357595920564, G_loss=1.5480903908610344\n",
            "Step=37899 D_loss=1.1531613385677337, G_loss=1.4491814467310906\n",
            "Step=37999 D_loss=0.8462584549188615, G_loss=1.3366589275002478\n",
            "Step=38099 D_loss=0.8681057792901994, G_loss=1.2371731612086294\n",
            "Step=38199 D_loss=1.1149604099988937, G_loss=1.613526042699814\n",
            "Step=38299 D_loss=0.8694253355264663, G_loss=1.9275622311234475\n",
            "Step=38399 D_loss=1.08740905046463, G_loss=1.2130165150761605\n",
            "Step=38499 D_loss=0.8837901240587234, G_loss=1.7323061984777453\n",
            "Step=38599 D_loss=1.1833230799436567, G_loss=1.145659102201462\n",
            "Step=38699 D_loss=0.755797837972641, G_loss=3.8457643291354175\n",
            "Step=38799 D_loss=1.4172809088230132, G_loss=1.8634726250171663\n",
            "Step=38899 D_loss=1.0351718240976333, G_loss=1.1838850483298302\n",
            "Step=38999 D_loss=1.2961311364173889, G_loss=1.2025172978639604\n",
            "Step=39099 D_loss=0.8600846457481385, G_loss=1.9616380143165586\n",
            "Step=39199 D_loss=1.3552416843175887, G_loss=1.3373365637660026\n",
            "Step=39299 D_loss=1.2632425773143767, G_loss=1.7633398020267488\n",
            "Step=39399 D_loss=0.6483873271942138, G_loss=2.004315464198589\n",
            "Step=39499 D_loss=1.1070277178287506, G_loss=4.625679897964001\n",
            "Step=39599 D_loss=0.6158928221464156, G_loss=6.069992897510529\n",
            "Step=39699 D_loss=0.964641716480255, G_loss=1.108734859228134\n",
            "Step=39799 D_loss=0.7094721427559852, G_loss=1.541782345175743\n",
            "Step=39899 D_loss=0.7974169608950614, G_loss=1.692378696799278\n",
            "Step=39999 D_loss=1.0408289620280267, G_loss=1.3717538577318191\n",
            "Step=40099 D_loss=1.3867257180809975, G_loss=1.64624592512846\n",
            "Step=40199 D_loss=0.5925300392508507, G_loss=1.7186703115701674\n",
            "Step=40299 D_loss=0.5599860870838165, G_loss=1.4289199167490008\n",
            "Step=40399 D_loss=0.9520806765556334, G_loss=1.9975885653495788\n",
            "Step=40499 D_loss=0.8569571024179459, G_loss=3.1235409834980965\n",
            "Step=40599 D_loss=1.0592547380924224, G_loss=1.3464663445949554\n",
            "Step=40699 D_loss=0.7250982362031937, G_loss=1.4597848710417747\n",
            "Step=40799 D_loss=1.1156435322761535, G_loss=2.0504688492417333\n",
            "Step=40899 D_loss=0.8842102509737015, G_loss=1.15627799898386\n",
            "Step=40999 D_loss=0.5581269225478174, G_loss=2.179398611485958\n",
            "Step=41099 D_loss=0.8212522521615029, G_loss=1.6000166791677473\n",
            "Step=41199 D_loss=1.5487407821416852, G_loss=1.377094325721264\n",
            "Step=41299 D_loss=1.1563998651504517, G_loss=3.073998727202415\n",
            "Step=41399 D_loss=1.232092229127884, G_loss=1.2454492303729057\n",
            "Step=41499 D_loss=0.8207660880684853, G_loss=1.5451800507307052\n",
            "Step=41599 D_loss=1.0552149298787117, G_loss=1.6368339064717292\n",
            "Step=41699 D_loss=1.0158767479658126, G_loss=1.9256955575942996\n",
            "Step=41799 D_loss=0.9941361111402511, G_loss=1.3719209608435632\n",
            "Step=41899 D_loss=0.8936447569727899, G_loss=2.047880397737026\n",
            "Step=41999 D_loss=1.0371463194489479, G_loss=1.684613384604454\n",
            "Step=42099 D_loss=1.1098405480384828, G_loss=1.137978051006794\n",
            "Step=42199 D_loss=0.7392873364686966, G_loss=1.3837551817297935\n",
            "Step=42299 D_loss=0.5182351779937744, G_loss=3.107422231435776\n",
            "Step=42399 D_loss=1.2275476795434952, G_loss=1.784378670156002\n",
            "Step=42499 D_loss=1.239617590904236, G_loss=1.9911416301131248\n",
            "Step=42599 D_loss=1.737985168993473, G_loss=1.2832174998521804\n",
            "Step=42699 D_loss=0.8851999941468238, G_loss=1.241623871922493\n",
            "Step=42799 D_loss=1.0896034976840019, G_loss=0.9129795131087302\n",
            "Step=42899 D_loss=0.5538679409027099, G_loss=1.8285614582896232\n",
            "Step=42999 D_loss=0.749621221423149, G_loss=2.3668661093711854\n",
            "Step=43099 D_loss=1.1640210753679274, G_loss=1.1583205941319465\n",
            "Step=43199 D_loss=1.1711598017811775, G_loss=1.4503972861170769\n",
            "Step=43299 D_loss=0.9422487118840217, G_loss=2.0617230612039563\n",
            "Step=43399 D_loss=1.2302609860897065, G_loss=2.0850316008925436\n",
            "Step=43499 D_loss=1.037584233880043, G_loss=1.896737939417362\n",
            "Step=43599 D_loss=0.8363444900512695, G_loss=1.2971341636776925\n",
            "Step=43699 D_loss=0.6385498574376107, G_loss=1.6157324036955834\n",
            "Step=43799 D_loss=0.6194887679815293, G_loss=2.1138123303651812\n",
            "Step=43899 D_loss=1.160663771033287, G_loss=5.375939670801163\n",
            "Step=43999 D_loss=1.307113167643547, G_loss=1.4581063333153725\n",
            "Step=44099 D_loss=0.9867973464727402, G_loss=1.7570573565363885\n",
            "Step=44199 D_loss=0.8773426315188406, G_loss=1.4599134373664857\n",
            "Step=44299 D_loss=0.9090120744705201, G_loss=1.5149179673194886\n",
            "Step=44399 D_loss=0.9248023337125778, G_loss=1.7900961154699324\n",
            "Step=44499 D_loss=0.8420937529206276, G_loss=1.9726956897974015\n",
            "Step=44599 D_loss=0.9336378839612007, G_loss=1.5782493314146995\n",
            "Step=44699 D_loss=0.7434106612205507, G_loss=3.2073802199959753\n",
            "Step=44799 D_loss=1.0110736933350561, G_loss=1.46626293361187\n",
            "Step=44899 D_loss=0.6514104297757148, G_loss=1.6682353624701503\n",
            "Step=44999 D_loss=0.950459752380848, G_loss=2.211526483297348\n",
            "Step=45099 D_loss=1.0708882418274879, G_loss=1.5789841780066487\n",
            "Step=45199 D_loss=1.0357378551363943, G_loss=1.1690589290857316\n",
            "Step=45299 D_loss=0.8936098521947862, G_loss=1.5124283015727997\n",
            "Step=45399 D_loss=0.6083175334334374, G_loss=1.6556069615483282\n",
            "Step=45499 D_loss=0.6222236642241478, G_loss=1.5597890916466715\n",
            "Step=45599 D_loss=0.7141576373577119, G_loss=2.0864497581124306\n",
            "Step=45699 D_loss=0.5161490786075593, G_loss=1.815708468556404\n",
            "Step=45799 D_loss=0.851559028327465, G_loss=2.240599637031555\n",
            "Step=45899 D_loss=0.6640098139643669, G_loss=1.9505200216174126\n",
            "Step=45999 D_loss=1.5690481033921242, G_loss=1.4598334324359894\n",
            "Step=46099 D_loss=0.8170909240841866, G_loss=1.4414553904533387\n",
            "Step=46199 D_loss=1.5905338394641875, G_loss=1.7576416313648224\n",
            "Step=46299 D_loss=0.74993497133255, G_loss=2.8912590304017067\n",
            "Step=46399 D_loss=1.1044957095384598, G_loss=1.053501582443714\n",
            "Step=46499 D_loss=0.8368166303634643, G_loss=1.184987675845623\n",
            "Step=46599 D_loss=1.1403362953662872, G_loss=1.4153825932741164\n",
            "Step=46699 D_loss=0.5600256934762, G_loss=1.8238896596431733\n",
            "Step=46799 D_loss=0.9583740639686584, G_loss=1.7881362324953076\n",
            "Step=46899 D_loss=1.3590332520008088, G_loss=2.0098260331153868\n",
            "Step=46999 D_loss=1.4691329231858254, G_loss=1.1057191929221153\n",
            "Step=47099 D_loss=0.8552906948328018, G_loss=1.5792098706960678\n",
            "Step=47199 D_loss=1.5744303804636002, G_loss=1.2271728286147117\n",
            "Step=47299 D_loss=1.0982307752966882, G_loss=1.695495919883251\n",
            "Step=47399 D_loss=0.8597429493069648, G_loss=2.27354463070631\n",
            "Step=47499 D_loss=0.7468886512517929, G_loss=2.0061603879928587\n",
            "Step=47599 D_loss=1.3747796195745468, G_loss=1.1864964863657952\n",
            "Step=47699 D_loss=0.6737021532654761, G_loss=1.6837859535217286\n",
            "Step=47799 D_loss=1.003193251490593, G_loss=1.617772822380066\n",
            "Step=47899 D_loss=1.442118508219719, G_loss=1.2665644386410715\n",
            "Step=47999 D_loss=0.8772138872742654, G_loss=2.212788173556328\n",
            "Step=48099 D_loss=1.1914516136050224, G_loss=2.774206263720989\n",
            "Step=48199 D_loss=1.3049998369812965, G_loss=1.0369073882699011\n",
            "Step=48299 D_loss=1.0996257337927817, G_loss=1.0164209008216858\n",
            "Step=48399 D_loss=0.6983485952019692, G_loss=1.4427235141396522\n",
            "Step=48499 D_loss=1.1822884824872015, G_loss=1.1509651803970338\n",
            "Step=48599 D_loss=0.9403199762105943, G_loss=1.7713531979918478\n",
            "Step=48699 D_loss=0.7966061508655548, G_loss=1.4942808416485787\n",
            "Step=48799 D_loss=1.0310444766283036, G_loss=1.5579532256722448\n",
            "Step=48899 D_loss=0.998231290280819, G_loss=1.417945347428322\n",
            "Step=48999 D_loss=1.0434816232323647, G_loss=1.1999415671825409\n",
            "Step=49099 D_loss=1.1086873817443847, G_loss=1.6254846286773683\n",
            "Step=49199 D_loss=1.1516211992502214, G_loss=1.0264275875687598\n",
            "Step=49299 D_loss=0.9246526354551317, G_loss=1.959450731575489\n",
            "Step=49399 D_loss=1.1406668207049369, G_loss=2.8448169627785687\n",
            "Step=49499 D_loss=1.5478420060873033, G_loss=1.0329645907878877\n",
            "Step=49599 D_loss=1.1681264027953149, G_loss=1.0974904236197471\n",
            "Step=49699 D_loss=1.1465591877698897, G_loss=1.1567387089133263\n",
            "Step=49799 D_loss=0.7827308055758475, G_loss=1.4896787598729133\n",
            "Step=49899 D_loss=1.1566190880537035, G_loss=1.0628731369972229\n",
            "Step=49999 D_loss=0.6669059941172599, G_loss=1.7067325878143311\n",
            "Reached 50001 epochs for GAN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGhiRZt02p7w"
      },
      "source": [
        "Move trained model to deployed model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9z8bZyU2nle",
        "outputId": "8b90b928-aded-4d1f-9034-02e99370617f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "if not os.path.exists(f\"{googlepath}deployed_models\"):\n",
        "\t    os.makedirs(f\"{googlepath}deployed_models\")\n",
        "\n",
        "copyfile(f\"{googlepath}models/gan.ckpt-9999.data-00000-of-00001\", f\"{googlepath}deployed_models/gan.data-00000-of-00001\") \n",
        "copyfile(f\"{googlepath}models/gan.ckpt-9999.index\", f\"{googlepath}deployed_models/gan.index\") \n",
        "copyfile(f\"{googlepath}models/gan.ckpt-9999.meta\", f\"{googlepath}deployed_models/gan.meta \") "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/MarketGAN/deployed_models/gan.meta '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQj9QRwNxiDX",
        "outputId": "fdb88402-9363-413f-a738-d5383182c58d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "##Predict mode\n",
        "#PREDICTING THE MODEL\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import xgboost as xgb\n",
        "from sklearn.externals import joblib\n",
        "#/deployed_models/gan'\n",
        "class Predict:\n",
        "  def __init__(self, num_historical_days=20, days=10, pct_change=0, gan_model=f'{googlepath}/deployed_models/gan', xgb_model=f'xgb'):\n",
        "    self.data = []\n",
        "    self.num_historical_days = num_historical_days\n",
        "    self.gan_model = gan_model\n",
        "    self.xgb_model = xgb_model\n",
        "       \n",
        "    files = [f\"{googlepath}stock_data/{f}\" for f in os.listdir(f\"{googlepath}stock_data\")] \n",
        "    for file in files:\n",
        "      \n",
        "      print(file)\n",
        "      df = pd.read_csv(file, index_col='timestamp', parse_dates=True)\n",
        "      df = df[['open','high','low','close','volume']]\n",
        "            # data for new column labels that will use the pct_change of the closing data.\n",
        "            # pct_change measure change between current and prior element. Map these into a 1x2\n",
        "            # array to show if the pct_change > (our desired threshold) or less than.\n",
        "            \n",
        "      df = ((df -\n",
        "            df.rolling(num_historical_days).mean().shift(-num_historical_days))\n",
        "            /(df.rolling(num_historical_days).max().shift(-num_historical_days)\n",
        "            -df.rolling(num_historical_days).min().shift(-num_historical_days)))\n",
        "      df = df.dropna()\n",
        "      self.data.append((file.split('/')[-1], df.iloc[0], df[200:200+num_historical_days].values))\n",
        "      #split the df into arrays of length num_historical_days and append\n",
        "      # to data, i.e. array of df[curr - num_days : curr] -> a batch of values\n",
        "      # appending if price went up or down in curr day of \"i\" we are lookin\n",
        "      # at\n",
        "      \n",
        "      \n",
        "  def gan_predict(self):\n",
        "    tf.reset_default_graph()\n",
        "    gan = GAN(num_features=5, num_historical_days=self.num_historical_days, generator_input_size=200, is_train=False)\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver = tf.train.Saver()\n",
        "      saver.restore(sess, self.gan_model)\n",
        "      clf = joblib.load(self.xgb_model)\n",
        "      for sym, date, data in self.data:\n",
        "        features = sess.run(gan.features, feed_dict={gan.X:[data]})\n",
        "        features = xgb.DMatrix(features)\n",
        "        print('{} {} {}'.format(str(date).split(' ')[0], sym, clf.predict(features)[0][1] > 0.5))\n",
        "        print(clf.predict(features))\n",
        "        #predictions = np.array([x for x in gan_estimator.predict(p.gan_predict())])\n",
        "        #print(predictions)\n",
        "\n",
        "p = Predict()\n",
        "p.gan_predict()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MarketGAN/stock_data/AAPL.csv\n",
            "/content/drive/My Drive/Colab Notebooks/MarketGAN/stock_data/ATVI.csv\n",
            "/content/drive/My Drive/Colab Notebooks/MarketGAN/stock_data/ADBE.csv\n",
            "Tensor(\"Relu:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "Tensor(\"Relu_4:0\", shape=(?, 20, 1, 32), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 20, 1, 64), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 18, 1, 128), dtype=float32)\n",
            "2304\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/Colab Notebooks/MarketGAN//deployed_models/gan\n",
            "open AAPL.csv True\n",
            "[[0.01988322 0.9801168 ]]\n",
            "0.9801168\n",
            "open ATVI.csv True\n",
            "[[0.01865298 0.98134696]]\n",
            "0.98134696\n",
            "open ADBE.csv True\n",
            "[[0.01855618 0.98144376]]\n",
            "0.98144376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jDR0mx7_734"
      },
      "source": [
        "[[down, up]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3tHSi0V7oXx"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/nmharmon8/StockMarketGAN/master/figures/XGB_GAN_Confusion_Matrix_Up_Or_Down_Over_10_Days_normalize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqr-JgUUT7R8"
      },
      "source": [
        "\n",
        "**Predictions of Up or Down movement over 10 Days The predictions over a 10 day period are quite good.**\n",
        "\n",
        "Testing The data the was held out in the training phase is run through the Discriminator portion of the GAN and the activated weights of the last convolutional layer are extracted. The extracted features are then classified using XGBoost.\n",
        "\n",
        "Results The confusion matrix shows the results of the model's classification. The perfect confusion matrix would only have predictions on the main diagonal. Each number off the main diagonal is a misclassification.\n",
        "\n",
        "**The up and down are indicating the movement of the stock price. So if the stock is going up we predict that it is going up 87% of the time and if it is going down we predict it 93% of the time which is great!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PvhVGi89lMF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}