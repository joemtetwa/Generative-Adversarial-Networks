{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvYDZmH_xHPG"
   },
   "outputs": [],
   "source": [
    "# PassGAN\n",
    "Based on paper [PassGAN: A Deep Learning Approach for Password Guessing](https://arxiv.org/abs/1709.00440)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outline\n",
    "- Introduction\n",
    "- Prerequest\n",
    "- Datasets\n",
    "- Build Models\n",
    "    - Generator Models\n",
    "    - Discriminator Models\n",
    "- Models Settings\n",
    "- Training\n",
    "- Result"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-d6053c93742e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d6053c93742e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### Introduction\n",
    "#### Abstract :<br>\n",
    "State-of-the-art password guessing tools, such as HashCat and John the Ripper, enable users to check billions of passwords per second against password hashes. In addition to performing straightforward dictionary attacks, these tools can expand password dictionaries using password generation rules, such as concatenation of words (e.g., \"password123456\") and leet speak (e.g., \"password\" becomes \"p4s5w0rd\"). Although these rules work well in practice, expanding them to model further passwords is a laborious task that requires specialized expertise. To address this issue, in this paper we introduce PassGAN, a novel approach that replaces human-generated password rules with theory-grounded machine learning algorithms. Instead of relying on manual password analysis, PassGAN uses a Generative Adversarial Network (GAN) to autonomously learn the distribution of real passwords from actual password leaks, and to generate high-quality password guesses. Our experiments show that this approach is very promising. When we evaluated PassGAN on two large password datasets, we were able to surpass rule-based and state-of-the-art machine learning password guessing tools. However, in contrast with the other tools, PassGAN achieved this result without any a-priori knowledge on passwords or common password structures. Additionally, when we combined the output of PassGAN with the output of HashCat, we were able to match 51%-73% more passwords than with HashCat alone. This is remarkable, because it shows that PassGAN can autonomously extract a considerable number of password properties that current state-of-the art rules do not encode. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prerequest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import All prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import os\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ROOT = \"password/\"\n",
    "\n",
    "# Make dir if no exist\n",
    "if not os.path.exists(ROOT):\n",
    "    os.makedirs(ROOT)\n",
    "\n",
    "# Download Library\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://raw.githubusercontent.com/DSC-UI-SRIN/Introduction-to-GAN/master/4%20-%20Applications%20of%20GANs/password/datasets.py\n",
    "\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://raw.githubusercontent.com/DSC-UI-SRIN/Introduction-to-GAN/master/4%20-%20Applications%20of%20GANs/password/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cu6vT-jDxHPT"
   },
   "outputs": [],
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "batch_size = 100\n",
    "\n",
    "# Rockyou Dataset\n",
    "\n",
    "train_dataset = datasets.Rockyou(root=ROOT, train=True, download=True, input_size=(10,0), tokenize=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLypGXB6xHPb"
   },
   "outputs": [],
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, functional\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=5):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dim, dim, padding=kernel_size//2, kernel_size=kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(dim, dim, padding=kernel_size//2, kernel_size=kernel_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = (self.model(input_data))\n",
    "        return input_data + output"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, seq_len, layer_dim, z_dim, char_len):\n",
    "        super(Generator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.layer_dim = layer_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.char_len = char_len\n",
    "\n",
    "        self.linear = nn.Linear(self.z_dim, self.seq_len*self.layer_dim)\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "        )\n",
    "        self.conv = nn.Conv1d(self.layer_dim, self.char_len, kernel_size=1)\n",
    "\n",
    "    def softmax(self, logits, num_classes):\n",
    "        logits = logits.reshape(-1, num_classes)\n",
    "        logits = logits.softmax(1)\n",
    "        return logits.reshape(-1, self.seq_len, self.char_len)\n",
    "\n",
    "    def forward(self, z_input):\n",
    "        output = self.linear(z_input)\n",
    "        output = output.view(-1, self.layer_dim, self.seq_len)\n",
    "        output = self.res_blocks(output)\n",
    "        output = self.conv(output)\n",
    "        output = output.permute([0, 2, 1])\n",
    "        output = self.softmax(output, self.char_len)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, seq_len, layer_dim, char_len):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.layer_dim = layer_dim\n",
    "        self.char_len = char_len\n",
    "\n",
    "        self.conv = nn.Conv1d(self.char_len, self.layer_dim, kernel_size=1)\n",
    "\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "            ResBlock(self.layer_dim),\n",
    "        )\n",
    "        self.linear = nn.Linear(self.seq_len*self.layer_dim, 1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        output = input_data.permute([0, 2, 1])\n",
    "        output = self.conv(output)\n",
    "        output = self.res_blocks(output)\n",
    "        output = output.view(-1, self.layer_dim*self.seq_len)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "z_dim = 128\n",
    "seq_len = 10\n",
    "layer_dim = 128\n",
    "\n",
    "\n",
    "G = Generator(seq_len, layer_dim, z_dim, len(train_dataset.class_to_idx)).to(device)\n",
    "D = Discriminator(seq_len, layer_dim, len(train_dataset.class_to_idx)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pw6SuIw-xHPm"
   },
   "outputs": [],
   "source": [
    "# Train Process\n",
    "\n",
    "![WGAN Algorithm](https://github.com/DSC-UI-SRIN/Introduction-to-GAN/raw/master/2%20-%20%20Wasserstein%20GANs/images/wgan-gp-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_data, fake_data):\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(\n",
    "        np.random.random((real_data.size(0), 1, 1)))\n",
    "\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "    d_interpolates = D(interpolates.requires_grad_(True))\n",
    "    fake = Tensor(real_data.shape[0], 1).fill_(1.0)\n",
    "\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    grads = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    grads = grads.reshape(grads.size(0), -1)\n",
    "    grad_penalty = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return grad_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "\n",
    "# optimizer\n",
    "lr = 1e-4\n",
    "n_critic =  5\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logdir = './runs'\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_generated_data(samples, iters, tag=\"result\"):\n",
    "    \"\"\"\n",
    "    this function used for check the result of generator network and save it to tensorboard\n",
    "    :param samples(dict): samples of input network\n",
    "    :param tag: save the output to tensorboard log wit tag\n",
    "    :param iters: global iteration counts for tensorboard logging\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        inv_charmap = train_dataset.idx_to_class\n",
    "\n",
    "        samples = G(samples)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            samples = samples.cpu().numpy()\n",
    "        else:\n",
    "            samples = samples.numpy()\n",
    "\n",
    "        samples = np.argmax(samples, axis=2)\n",
    "\n",
    "        decoded_samples = []\n",
    "        for i in range(len(samples)):\n",
    "            decoded = []\n",
    "            for j in range(len(samples[i])):\n",
    "                decoded.append(inv_charmap[samples[i][j]])\n",
    "            decoded_samples.append(\"\".join(decoded).replace('`', \"\"))\n",
    "        # print(\", \".join(decoded_samples))\n",
    "        writer.add_text(tag, \", \".join(decoded_samples), iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "list_loss_D = []\n",
    "list_loss_G = []\n",
    "fixed_z = Variable(Tensor(np.random.normal(0, 1, (10, z_dim))))\n",
    "for epoch in range(epochs):\n",
    "    for i, (X, _) in enumerate(train_loader):\n",
    "        # Configure input\n",
    "        real_data = Variable(X.type(Tensor))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = Variable(Tensor(np.random.normal(0, 1, (real_data.shape[0], z_dim))))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        fake_data = G(z).detach()\n",
    "\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(D, real_data.data, fake_data.data)\n",
    "\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(D(real_data)) + torch.mean(D(fake_data)) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator every n_critic iterations\n",
    "        if i % n_critic == 0:\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_data = G(z)\n",
    "            # Adversarial loss\n",
    "            g_loss = -torch.mean(D(gen_data))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            list_loss_D.append(d_loss.item())\n",
    "            list_loss_G.append(g_loss.item())\n",
    "        \n",
    "        if i % 300 == 0:\n",
    "            print(\n",
    "              \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "              % (epoch, epochs, i, len(train_loader), d_loss.item(), g_loss.item()))\n",
    "            writer.add_scalar('G_loss', g_loss.item(), epoch * len(train_loader) + i)\n",
    "            writer.add_scalar('D_loss', d_loss.item(), epoch * len(train_loader) + i)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        check_generated_data(fixed_z, tag=\"result_{}\".format(epoch), iters=epoch * len(train_loader) + i)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}